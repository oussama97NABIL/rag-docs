# RAGÂ Docs â€“ Questionâ€RÃ©ponse sur des documents

**RAGÂ Docs** est un projet pÃ©dagogique qui montre comment mettre en place un
pipeline *Retrievalâ€‘Augmented Generation* (RAG) pour interroger un
corpus de documents.  Lâ€™application vous permet de poser des
questions en langage naturel et dâ€™obtenir des rÃ©ponses Ã©tayÃ©es par
les passages pertinents des documents.  Elle se composeÂ dâ€™un
backend en Python/Flask et dâ€™une interface lÃ©gÃ¨re en Streamlit.

## âœ¨Â Quâ€™estâ€‘ce que le RAGÂ ?

Le RAG associe deux techniquesÂ : la recherche sÃ©mantique et la
gÃ©nÃ©ration par un modÃ¨le de langage.  Un pipeline typique suit ces
Ã©tapesÂ :

1. **Conversion de la requÃªte en vecteurÂ :** la question de
   lâ€™utilisateur est transformÃ©e en un *embedding* dense.
2. **Recherche de contextes pertinentsÂ :** on interroge un index de
   vecteurs pour rÃ©cupÃ©rer les passages de documents les plus proches
   sÃ©mantiquementã€741912313199305â€ L181-L191ã€‘.
3. **GÃ©nÃ©rationÂ :** on fournit ces passages comme contexte au modÃ¨le de
   langage, qui produit une rÃ©ponse en se basant Ã  la fois sur ses
   paramÃ¨tres internes et sur les informations rÃ©cupÃ©rÃ©esã€741912313199305â€ L181-L191ã€‘.

Ce schÃ©ma amÃ©liore la fiabilitÃ© des rÃ©ponsesÂ : il rÃ©duit les
hallucinations, permet de fournir des sources et met Ã  jour les
connaissances sans rÃ©entraÃ®ner le modÃ¨leã€741912313199305â€ L181-L196ã€‘.  Une
implÃ©mentation minimale consiste Ã Â : charger des documents, les
dÃ©couper en morceaux, calculer des embeddings via
SentenceTransformers, les indexer avec FAISS, recevoir une requÃªte,
rÃ©cupÃ©rer les *topÂ K* morceaux et interroger un LLMã€604453273349529â€ L59-L64ã€‘.

## ğŸ“Â Structure du projet

Le dÃ©pÃ´t suit lâ€™arborescence recommandÃ©e ciâ€‘dessousÂ :

```text
rag-docs/
â”œâ”€â”€ backend/            # API Flask et logique RAG
â”‚   â”œâ”€â”€ main.py         # EntrÃ©e principale de lâ€™API
â”‚   â”œâ”€â”€ models.py       # SchÃ©mas Pydantic pour les requÃªtes/rÃ©ponses
â”‚   â”œâ”€â”€ rag_engine.py   # Pipeline RAG (parsing, embedding, retrieval, gÃ©nÃ©ration)
â”‚   â””â”€â”€ requirements.txt# DÃ©pendances Python
â”œâ”€â”€ frontend/           # Interface utilisateur Streamlit
â”‚   â””â”€â”€ app.py         # Application Streamlit
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw_documents/  # Placez ici vos PDF/DOCX/TXT
â””â”€â”€ README.md
```

## ğŸš€Â Installation

1. **Clonez** le dÃ©pÃ´t et placezâ€‘vous Ã  la racineÂ :

   ```bash
   git clone <votreâ€‘urlâ€‘deâ€‘repo>
   cd rag-docs
   ```

2. **CrÃ©ez un environnement virtuel** et installez les dÃ©pendancesÂ :

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   pip install -r backend/requirements.txt
   pip install streamlit
   ```

3. **PrÃ©parez vos documentsÂ :**

   - TÃ©lÃ©chargez les fichiers PDF/DOCX/TXT depuis le [GoogleÂ Drive fourni](https://drive.google.com/drive/folders/1Mt0Z4yLhOfeDo-1sQMb5IpX-__QwcV-h?usp=sharing).
   - Copiez-les dans le dossier `data/raw_documents/`.

4. (Facultatif) **Configurez une clÃ© OpenAIÂ :**

   - Pour obtenir des rÃ©ponses gÃ©nÃ©rÃ©es par GPTâ€‘3.5/4, crÃ©ez une clÃ© sur
     [platform.openai.com](https://platform.openai.com/) et exportez la variable
     dâ€™environnement `OPENAI_API_KEY`Â :

     ```bash
     export OPENAI_API_KEY="sk-..."
     # OptionnelÂ : choisir le modÃ¨le (gpt-3.5-turbo, gpt-4, etc.)
     export OPENAI_MODEL="gpt-3.5-turbo"
     ```

   - Sans clÃ©, lâ€™API retournera simplement le texte du passage le plus
     pertinent en guise de rÃ©ponse.

## ğŸ–¥ï¸Â Lancement du backend

Lâ€™API Flask fournit deux routesÂ :

| Route            | MÃ©thode | Description                                  |
|------------------|---------|----------------------------------------------|
| `/healthcheck`   | GET     | VÃ©rifie que le service est en ligne          |
| `/ask`           | POST    | Accepte `{"question": "..."}` et renvoie une rÃ©ponse JSON |

Pour dÃ©marrer le serveurÂ :

```bash
cd backend
python main.py
```

Le serveur lit automatiquement les documents, calcule les embeddings
et construit lâ€™index FAISSã€604453273349529â€ L59-L64ã€‘.  Ces Ã©tapes peuvent prendre
quelques minutes selon la taille du corpus.  Vous pouvez personnaliser
le nombre de documents rÃ©cupÃ©rÃ©s en modifiant le paramÃ¨tre `top_k`
dans `rag_engine.py`.

## ğŸ’¬Â Lancement de lâ€™interface

Lâ€™interface Streamlit permet de poser une question et dâ€™afficher la
rÃ©ponse avec les sources.  Lancezâ€‘la dans un deuxiÃ¨me terminalÂ :

```bash
cd frontend
streamlit run app.py
```

Par dÃ©faut, lâ€™interface sâ€™attend Ã  ce que le backend tourne sur
`http://localhost:8000`.  Pour utiliser une adresse diffÃ©rente (par
exemple en production), dÃ©finissez la variable `BACKEND_URL`Â :

```bash
export BACKEND_URL="https://mon-serveur:8000"
streamlit run app.py
```

## ğŸ§ªÂ Fonctionnement interne

1. **Parsing des documentsÂ :**
   - Les fichiers PDF sont extraits avec **PyPDF2** et les DOCX avec
     **pythonâ€‘docx**.  Les textes bruts sont nettoyÃ©s (espaces et
     retours Ã  la ligne).
2. **DÃ©coupage en chunksÂ :**
   - Chaque document est dÃ©coupÃ© en segments de 500 mots avec un
     chevauchement de 50 mots.  Ce dÃ©coupage permet de conserver le
     contexte tout en restant compatible avec la longueur maximale des
     modÃ¨les de langage.
3. **GÃ©nÃ©ration des embeddingsÂ :**
   - Les morceaux sont encodÃ©s avec le modÃ¨le **allâ€‘MiniLMâ€‘L6â€‘v2** du
     package `sentence-transformers`, qui offre un bon compromis entre
     vitesse et qualitÃ©ã€741912313199305â€ L246-L268ã€‘.  Les vecteurs
     rÃ©sultants sont de dimension fixe.
4. **Indexation FAISSÂ :**
   - Les embeddings sont stockÃ©s dans un index **FAISS** de type
     `IndexFlatL2`.  FAISS est un moteur de recherche de similaritÃ©
     efficace et open source dÃ©veloppÃ© par MetaÂ AIã€741912313199305â€ L151-L155ã€‘.
5. **RequÃªte utilisateurÂ :**
   - La question est encodÃ©e en vecteur et lâ€™index renvoie les `top_k`
     passages les plus proches en termes de distance euclidienneã€741912313199305â€ L181-L191ã€‘.
6. **GÃ©nÃ©rationÂ :**
   - Si une clÃ© OpenAI est disponible, ces passages sont injectÃ©s dans
     un *prompt* envoyÃ© Ã  `openai.ChatCompletion.create`.  Sinon
     lâ€™application retourne le premier passage comme rÃ©ponse.

## ğŸ§ Â CritÃ¨res dâ€™Ã©valuation

Le projet rÃ©pond aux exigences suivantesÂ :

| CritÃ¨re              | DÃ©tail                                                               |
|----------------------|----------------------------------------------------------------------|
| **FonctionnalitÃ©**   | Les documents sont dÃ©coupÃ©s, indexÃ©s et interrogÃ©s de faÃ§on robuste. |
| **Usage du LLM**     | Le LLM nâ€™est sollicitÃ© que pour la gÃ©nÃ©ration finale. Les sources sont renvoyÃ©es. |
| **Structuration**    | Le code est organisÃ© (backend, frontend, moteur), use Pydantic pour valider les requÃªtes. |
| **Interface**        | Lâ€™UI Streamlit est minimale, lisible et montre les rÃ©ponses et les sources. |
| **README**           | Ce document fournit des instructions complÃ¨tes pour reproduire lâ€™application. |
| **Bonus**            | La structure du code permet dâ€™ajouter un historique, un upload dynamique ou des tests unitaires. |

## ğŸ§ªÂ Exemple dâ€™utilisation

AprÃ¨s avoir lancÃ© le backend et lâ€™interface, posez une questionÂ :

```text
Votre questionÂ : Qu'estâ€‘ce que FAISSÂ ?

RÃ©ponseÂ : FAISS est une bibliothÃ¨que open source dÃ©veloppÃ©e par Facebook AI Research pour lâ€™indexation et la recherche de similaritÃ© sur des vecteurs. Elle est utilisÃ©e pour effectuer des recherches rapides dans des bases de vecteursã€741912313199305â€ L149-L156ã€‘.

SourcesÂ :
- vector_databases.txt â€“ "FAISS spÃ©cifiquement a Ã©tÃ© dÃ©veloppÃ© par FacebookÂ AI Research et est une bibliothÃ¨que open source pour une recherche de similaritÃ© efficace"ã€741912313199305â€ L149-L156ã€‘
```

## ğŸ› ï¸Â Personnalisation

* **Changer la taille des chunksÂ :** modifiez `chunk_size` et
  `chunk_overlap` lors de lâ€™instanciation du `RAGEngine` dans
  `backend/rag_engine.py`.
* **Augmenter le nombre de passages rÃ©cupÃ©rÃ©sÂ :** passez un autre
  `top_k` Ã  `answer_question`.
* **Changer de modÃ¨le dâ€™embeddingÂ :** remplacez
  `all-MiniLM-L6-v2` par un autre modÃ¨le SentenceTransformers.
* **Utiliser un LLM localÂ :** vous pouvez remplacer lâ€™appel Ã  OpenAI
  par un modÃ¨le HuggingÂ Face (voir lâ€™implÃ©mentation de la fonction
  `rag_response` dans le tutoriel MarkTechPost pour un exempleã€741912313199305â€ L340-L395ã€‘).

---

Ce projet est fourni Ã  des fins Ã©ducatives pour illustrer la mise en
place dâ€™un systÃ¨me de gÃ©nÃ©ration augmentÃ©e par la recherche.  Nâ€™hÃ©sitez
pas Ã  lâ€™adapter et lâ€™Ã©tendre selon vos besoinsÂ !